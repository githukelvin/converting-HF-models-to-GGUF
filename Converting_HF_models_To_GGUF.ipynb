{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlJCuDJFiMDx2i1CaFSgGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githukelvin/converting-HF-models-to-GGUF/blob/main/Converting_HF_models_To_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains code and blocks to convert a hf models to GGUF"
      ],
      "metadata": {
        "id": "Mn44z9A_WUKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOcptUkIUiGl"
      },
      "outputs": [],
      "source": [
        "# First clone llama.cpp git repository\n",
        "!!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step is to make the the repo and install requiremenst inrequirement.txt\n",
        "\n",
        "!cd llama.cpp && LLAMA_CUBLAS=1 make && pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "It94ytz_WlCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to huggingFace to allow you to download models and push the gguf to your repo\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "aGcJiKvsX4Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use Snapshot download from hugging face libray\n",
        "from huggingface_hub import snapshot_download"
      ],
      "metadata": {
        "id": "9wN4nqG0YIUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fisrt is the model you want to convert to gguf\n",
        "# method type 8,4,16 bit quatinzation\n",
        "# Location of where the model to be stored\n",
        "# Location of where the quantized model to be stored\n",
        "model_name=\"Jacaranda/UlizaLlama\"\n",
        "methods= [\n",
        "   \"q4_k_m\"\n",
        "]\n",
        "base_model=\"./original_model\"\n",
        "quantized_path=\"./quantized_model\""
      ],
      "metadata": {
        "id": "AoyU5RWSYSae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using snapshot download the model\n",
        "snapshot_download(repo_id=model_name, revision=\"main\", local_dir=base_model,local_dir_use_symlinks=False)"
      ],
      "metadata": {
        "id": "tbgghR1QY05K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model=quantized_path+\"FP16.gguf\""
      ],
      "metadata": {
        "id": "auC4OG1uY7LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make directory to store the model\n",
        "!mkdir ./quantized_model"
      ],
      "metadata": {
        "id": "AhEwgJwLZAdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code line when executed is used to convert the model to gguf\n",
        "!python llama.cpp/convert-hf-to-gguf.py ./original_model/ --outtype f16 --outfile ./quantized_model/FP16.gguf"
      ],
      "metadata": {
        "id": "d9gay4zEZQLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell simply renames all file acording to methods passed\n",
        "\n",
        "import os\n",
        "for m in methods:\n",
        "  qtype = f\"{quantized_path}/{m.upper()}.gguf\"\n",
        "  os.system(\"./llama.cpp/quantize \"+quantized_path+\"/FP16.gguf \"+qtype+\" \"+m)"
      ],
      "metadata": {
        "id": "K8hHhMQeZaQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for testing the gguf model\n",
        "!./llama.cpp/main -m ./quantized_model/Q4_K_M.gguf -n 90 --repeat_penalty 1.0 --color -i -r \"User:\" -f llama.cpp/prompts/chat-with-bob-sw.txt"
      ],
      "metadata": {
        "id": "RhKNN_WfZuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Uploading the gguf  to hugging face import necessary api\n",
        "from huggingface_hub import HfApi,HfFolder,create_repo,upload_file"
      ],
      "metadata": {
        "id": "_LdXYkZlaBuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path=\"./quantized_model/Q4_K_M.gguf\"\n",
        "repo_name=\"UlizaLlama_Q4_K_M-gguf\"\n",
        "repo_url=create_repo(repo_name,exist_ok=True,private=False)"
      ],
      "metadata": {
        "id": "BQmI2QXPaOGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api= HfApi()"
      ],
      "metadata": {
        "id": "Ejder_YkaRFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now pushing the model to huggingface\n",
        "api.upload_file(\n",
        "    path_or_fileobj=model_path,\n",
        "    path_in_repo=\"Q4_K_M.gguf\",\n",
        "    repo_id=\"de-coder/UlizaLlama_Q4_K_M-gguf\",\n",
        "    repo_type=\"model\"\n",
        "    )"
      ],
      "metadata": {
        "id": "HWrDoDvnaT-y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}